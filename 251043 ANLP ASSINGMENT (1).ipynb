{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6cfd0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 files in the training directory: /Users/sagar/Downloads/lab2resources 2/sentence-completion\n"
     ]
    }
   ],
   "source": [
    "import os,random,math\n",
    "TRAINING_DIR=\"/Users/sagar/Downloads/lab2resources 2/sentence-completion\"  #this needs to be the parent directory for the training corpus\n",
    "\n",
    "def get_training_testing(training_dir=TRAINING_DIR,split=0.5):\n",
    "\n",
    "    filenames=os.listdir(training_dir)\n",
    "    n=len(filenames)\n",
    "    print(\"There are {} files in the training directory: {}\".format(n,training_dir))\n",
    "    #random.seed(53)  #if you want the same random split every time\n",
    "    random.shuffle(filenames)\n",
    "    index=int(n*split)\n",
    "    return(filenames[:index],filenames[index:])\n",
    "\n",
    "trainingfiles,heldoutfiles=get_training_testing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9649a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sagar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/sagar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import operator\n",
    "import nltk \n",
    "from nltk import word_tokenize as tokenize\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "from nltk.stem import \tWordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a452945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2084b899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/sagar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "import string\n",
    "\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "punc = string.punctuation\n",
    "\n",
    "# Create stopword + punctuation list.\n",
    "stop_puncs = (set([x for x in punc] + list(stop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a3d23d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_testing(training_dir,split=0.5):\n",
    "\n",
    "    filenames=os.listdir(training_dir)\n",
    "    n=len(filenames)\n",
    "    print(\"There are {} files in the training directory: {}\".format(n,training_dir))\n",
    "    # random.seed(53)  #if you want the same random split every time\n",
    "    random.shuffle(filenames)\n",
    "    index=int(n*split)\n",
    "    trainingfiles=filenames[:index]\n",
    "    heldoutfiles=filenames[index:]\n",
    "    return trainingfiles,heldoutfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28a7124b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 522 files in the training directory: /Users/sagar/Downloads/lab2resources 2/sentence-completion/Holmes_Training_Data\n"
     ]
    }
   ],
   "source": [
    "parentdir=\"/Users/sagar/Downloads/lab2resources 2/sentence-completion\"\n",
    "trainingdir=os.path.join(parentdir,\"Holmes_Training_Data\")\n",
    "training,testing=get_training_testing(trainingdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfb529db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class n_gram_language_model():\n",
    "\n",
    "    \"\"\"\n",
    "    N-gram language model class that stores n-grams and their probabilties learnt from training text\n",
    "    in individiual dictionaries.\n",
    "\n",
    "    Code Taken from the original work of Dr. J Weeds, University of Sussex.   \n",
    "   \n",
    "    Parameters\n",
    "    ----------\n",
    "    trainingdir : str\n",
    "        The training directory where training data can be found.\n",
    "    files : list\n",
    "        List of file names to be trained on.\n",
    "    test_files : list\n",
    "        List of file names for the model to be tested on.\n",
    "    construct_params : dict\n",
    "        Stores the parameters such as known to initialize the language model with.\n",
    "    Attributes\n",
    "    ----------\n",
    "    trainingdir : str\n",
    "        The training directory where training data can be found.\n",
    "    files : list\n",
    "        List of file names to be trained on.\n",
    "    test_files : list\n",
    "        List of file names for the model to be tested on.\n",
    "    construct_params : dict\n",
    "        Stores the parameters such as known to initialize the language model with.\n",
    "    verbose : bool\n",
    "        Whether or not method calls will print progress.\n",
    "    unigram : dict\n",
    "        Dictionary to store unigram probabilities.\n",
    "    bigram : dict\n",
    "        Dictionary to store bigram probabilities.\n",
    "    trigram : dict\n",
    "        Dictionary to store trigram probabilities.\n",
    "    4-gram : dict\n",
    "        Dictionary to store 4-gram probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def __init__(self,trainingdir,files=[], test_files=[], construct_params={}):\n",
    "        self.training_dir=trainingdir\n",
    "        self.files=files\n",
    "        self.test_files = test_files\n",
    "        # Constructor Parameters.\n",
    "        self.construct_params=construct_params\n",
    "        self.verbose = construct_params.get(\"verbose\", False)\n",
    "        self.train()\n",
    "        \n",
    "    def train(self):    \n",
    "        \n",
    "        self.unigram={}\n",
    "        self.bigram={}\n",
    "        self.trigram={}\n",
    "        self.quad_gram={}\n",
    "         \n",
    "        self._processfiles()\n",
    "        self._make_unknowns(known=self.construct_params.get(\"known\",2))\n",
    "        self._discount()\n",
    "        self._convert_to_probs()\n",
    "\n",
    "        \n",
    "    \n",
    "    def _processline(self,line):\n",
    "       \n",
    "        tokens=tokenize(line)\n",
    "        if self.construct_params.get(\"remove_stopwords\",False) == True:\n",
    "          tokens = [token.lower() for token in tokens if token.lower() not in stop_puncs]\n",
    "        if self.construct_params.get(\"lemmatize\", False) == True:\n",
    "          tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "        tokens = [\"__START\"] + tokens + [\"__END\"]\n",
    "        previous=\"__END\"\n",
    "        for i, token in enumerate(tokens):\n",
    "            # Unigram\n",
    "            self.unigram[token]=self.unigram.get(token,0)+1\n",
    "            # Bigram\n",
    "            current=self.bigram.get(previous,{})\n",
    "            current[token]=current.get(token,0)+1\n",
    "            self.bigram[previous]=current\n",
    "            previous=token\n",
    "            # Trigram\n",
    "            if i < len(tokens)-2:\n",
    "              # Next words.\n",
    "              next = tokens[i+1] \n",
    "              next_next = tokens[i+2]\n",
    "              # Get dictionaries.\n",
    "              inner = self.trigram.get(token,{})\n",
    "              innermost = inner.get(next,{})\n",
    "              innermost[next_next] = innermost.get(token,0) + 1\n",
    "              # Write frequencies to dictionaries.\n",
    "              inner[next] = innermost\n",
    "              self.trigram[token] = inner\n",
    "            # 4-gram\n",
    "            if i < len(tokens)-3:\n",
    "              # Next words.\n",
    "              next1 = tokens[i+1] \n",
    "              next2 = tokens[i+2]\n",
    "              next3 = tokens[i+3]\n",
    "              # Get dictionaries.\n",
    "              inner1 = self.quad_gram.get(token,{})\n",
    "              inner2 = inner1.get(next1,{})\n",
    "              inner3 = inner2.get(next2,{})\n",
    "              inner3[next3] = inner3.get(token,0) + 1\n",
    "              # Write frequencies to dictionaries.\n",
    "              inner2[next2] = inner3\n",
    "              inner1[next1] = inner2\n",
    "              self.quad_gram[token] = inner1\n",
    "\n",
    "                      \n",
    "            \n",
    "    def _processfiles(self):\n",
    "        \"\"\"\n",
    "        Process text files.\n",
    "        \"\"\"\n",
    "        for afile in tqdm.tqdm(self.files):\n",
    "            # print(\"Processing {}\".format(afile))\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line=line.rstrip()\n",
    "                        if len(line)>0:\n",
    "                            self._processline(line)\n",
    "            except UnicodeDecodeError:\n",
    "              if self.verbose:\n",
    "                print(\"UnicodeDecodeError processing {}: ignoring rest of file\".format(afile))\n",
    "              else:\n",
    "                pass\n",
    "      \n",
    "            \n",
    "    def _convert_to_probs(self):\n",
    "        \n",
    "        self.unigram={k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()}\n",
    "        self.bigram={key:{k:v/sum(adict.values()) for (k,v) in adict.items()} for (key,adict) in self.bigram.items()}\n",
    "        self.trigram={k1:{k2:{k3:v/sum(adict2.values()) for k3, v in adict2.items()} for k2, adict2 in adict1.items()} for k1, adict1 in self.trigram.items()}\n",
    "        self.quad_gram={k1:{k2:{k3:{k4:v/sum(adict3.values()) for k4, v in adict3.items()} for k3, adict3 in adict2.items()} for k2, adict2 in adict1.items()} for k1, adict1 in self.quad_gram.items()}\n",
    "        self.kn={k:v/sum(self.kn.values()) for (k,v) in self.kn.items()}\n",
    "    \n",
    "\n",
    "    def nextlikely(self,k=1,current=\"\",method=\"unigram\"):\n",
    "        #use probabilities according to method to generate a likely next sequence\n",
    "        #choose random token from k best\n",
    "        blacklist=[\"__START\",\"__UNK\",\"__DISCOUNT\"]\n",
    "        most_likely = []\n",
    "        if method==\"unigram\":\n",
    "            dist=self.unigram\n",
    "            #sort the tokens by unigram probability\n",
    "            most_likely=sorted(list(dist.items()),key=operator.itemgetter(1),reverse=True)\n",
    "        elif method == \"bigram\":\n",
    "            dist=self.bigram.get(current,self.bigram.get(\"__UNK\",{}))\n",
    "            most_likely=sorted(list(dist.items()),key=operator.itemgetter(1),reverse=True)\n",
    "        elif method == \"trigram\":\n",
    "            # Split context string for first and second context words.\n",
    "            context = current.split()\n",
    "            c1, c2 = context[0], context[1]\n",
    "            dist = self.trigram[c1][c2]\n",
    "            # Get all words with maximum value.\n",
    "            most_likely = [(k, _) for k, v in dist.items() if v == max(dist.values())]\n",
    "        elif method == \"quad_gram\":\n",
    "            context = current.split(\" \")\n",
    "            c1,c2,c3 = context[0], context[1], context[2]\n",
    "            dist = self.quad_gram[c1][c2][c3]\n",
    "            most_likely = [(k, _) for k, v in dist.items() if v == max(dist.values())]\n",
    "        #filter out any undesirable tokens\n",
    "        filtered=[w for (w,p) in most_likely if w not in blacklist]\n",
    "        #choose one randomly from the top k\n",
    "        res=random.choice(filtered[:k])\n",
    "        return res\n",
    "    \n",
    "    def generate(self,k=3,end=\"__END\",limit=20,method=\"bigram\",methodparams={}):\n",
    "        \n",
    "        if method==\"\":\n",
    "            method=methodparams.get(\"method\",\"bigram\")\n",
    "        current=\"__START\"\n",
    "        tokens=[]\n",
    "        try: \n",
    "          # Trigram\n",
    "          if method==\"trigram\":\n",
    "            # Set current word to first context.\n",
    "            context_1 = current\n",
    "            # Set random choice of next word to second context.\n",
    "            context_2 = random.choice([key for key, adict in self.trigram[current].items()])\n",
    "            # Check end token hasnt been reached.\n",
    "            while context_2 != end and len(tokens)<limit:\n",
    "              # Pass current contexts to next likely method which re splits them in the tri- 4-gram cases.\n",
    "              current = \" \".join([context_1, context_2])\n",
    "              current = self.nextlikely(k=k, current=current, method=method)\n",
    "              # Append word to the list that will eventually be generated.\n",
    "              tokens.append(current)\n",
    "              # Set the the second context to now be first and the predicted word (current) to be next.\n",
    "              context_1 = context_2\n",
    "              context_2 = current\n",
    "            # After loop return the tokens joined by whitespace.\n",
    "            return \" \".join(tokens[:-1])\n",
    "          # Quad-Gram\n",
    "          elif method == \"quad_gram\":\n",
    "            # Functionality is the same as above with an additional context variable to account for 4 rather than 3 n-grams.\n",
    "            context_1 = current\n",
    "            context_2 = random.choice([key for key, adict in self.quad_gram[context_1].items()])\n",
    "            context_3 = random.choice([key for key, adict in self.quad_gram[context_1][context_2].items()])\n",
    "            while context_3 != end and len(tokens) < limit: \n",
    "              current = \" \".join([context_1, context_2, context_3])\n",
    "              current = self.nextlikely(k=k, current=current, method=method)\n",
    "              tokens.append(current)\n",
    "              context_1 = context_2\n",
    "              context_2 = context_3\n",
    "              context_3 = current\n",
    "            return \" \".join(tokens[:-1])\n",
    "        except:\n",
    "          # If error is thrown rerun method until it generates a valid sentence.\n",
    "          return self.generate(k=k,end=end,limit=limit,method=method,methodparams=methodparams)\n",
    "        # Below calls the unigram and bigram versions of the method.\n",
    "        while current!=end and len(tokens)<limit:\n",
    "            current=self.nextlikely(k=k,current=current,method=method)\n",
    "            tokens.append(current)\n",
    "        return \" \".join(tokens[:-1])\n",
    "    \n",
    "    \n",
    "    def get_prob(self,token,context=\"\",methodparams={}):\n",
    "        if methodparams.get(\"method\",\"unigram\")==\"unigram\":\n",
    "            return self.unigram.get(token,self.unigram.get(\"__UNK\",0))\n",
    "        else:\n",
    "            if methodparams.get(\"smoothing\",\"kneser-ney\")==\"kneser-ney\":\n",
    "                unidist=self.kn\n",
    "            else:\n",
    "                unidist=self.unigram\n",
    "            bigram=self.bigram.get(context[-1],self.bigram.get(\"__UNK\",{}))\n",
    "            big_p=bigram.get(token,bigram.get(\"__UNK\",0))\n",
    "            lmbda=bigram[\"__DISCOUNT\"]\n",
    "            uni_p=unidist.get(token,unidist.get(\"__UNK\",0))\n",
    "            #print(big_p,lmbda,uni_p)\n",
    "            p=big_p+lmbda*uni_p            \n",
    "            return p\n",
    "    \n",
    "    \n",
    "    def compute_prob_line(self,line,methodparams={}):\n",
    "        \n",
    "        tokens=tokenize(line)\n",
    "        if self.construct_params.get(\"remove_stopwords\",False) == True:\n",
    "          tokens = [token.lower() for token in tokens if token.lower() not in stop_puncs]\n",
    "        if self.construct_params.get(\"lemmatize\", False) == True:\n",
    "          tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "        tokens = [\"__START\"] + tokens + [\"__END\"]\n",
    "        acc=0\n",
    "        if methodparams.get(\"method\", \"unigram\") in [\"unigram\", \"bigram\"]:\n",
    "          for i,token in enumerate(tokens[1:]):\n",
    "            acc+=math.log(self.get_prob(token,tokens[:i+1],methodparams))\n",
    "          return acc,len(tokens[1:])\n",
    "        # Trigram. \n",
    "        if methodparams.get(\"method\") == \"trigram\":\n",
    "          try:\n",
    "            for i, token in enumerate(tokens[1:]):\n",
    "              if i < len(tokens[1:]) - 3 and len(tokens[1:]) >= 3:\n",
    "                word1, word2, word3 = tokens[i+1], tokens[i+1+1], tokens[i+1+2]\n",
    "                if word1 in self.trigram:\n",
    "                  if word2 in self.trigram[word1]:\n",
    "                    if word3 in self.trigram[word1][word2]:\n",
    "                      acc+=math.log(self.trigram[word1][word2][word3])\n",
    "                    else:\n",
    "                      acc+=math.log(self.trigram[word1][word2][\"__UNK\"])\n",
    "                  else:\n",
    "                    if word3 in self.trigram[word1][\"__UNK\"]:\n",
    "                      acc+=math.log(self.trigram[word1][\"__UNK\"][word3])\n",
    "                    else: \n",
    "                      acc+=math.log(self.trigram[word1][\"__UNK\"][\"__UNK\"])\n",
    "                else:\n",
    "                  if word2 in self.trigram[\"__UNK\"]:\n",
    "                    if word3 in self.trigram[\"__UNK\"][word2]:\n",
    "                      acc+=math.log(self.trigram[\"__UNK\"][word2][word3])\n",
    "                    else:\n",
    "                      acc+=math.log(self.trigram[\"__UNK\"][word2][\"__UNK\"])\n",
    "                  else:\n",
    "                    if word3 in self.trigram[\"__UNK\"][\"__UNK\"]:\n",
    "                      acc+=math.log(self.trigram[\"__UNK\"][\"__UNK\"][word3])\n",
    "                    else:\n",
    "                      acc+=math.log(self.trigram[\"__UNK\"][\"__UNK\"][\"__UNK\"])\n",
    "            return acc, len(tokens[1:])\n",
    "          except KeyError:\n",
    "            return acc, len(tokens[1:]) \n",
    "        # Quad_gram - same as above. FYI - if else if statements are used rather than if elif to enhance readability.\n",
    "        if methodparams.get(\"method\") == \"quad_gram\":\n",
    "          try:\n",
    "            for i, token in enumerate(tokens[1:]):\n",
    "              if i < len(tokens[1:]) - 4 and len(tokens[1:]) >= 4:\n",
    "                word1, word2, word3, word4 = tokens[i+1], tokens[i+1+1], tokens[i+1+2], tokens[i+1+3]\n",
    "                if word1 in self.quad_gram:\n",
    "                  if word2 in self.quad_gram[word1]:\n",
    "                    if word3 in self.quad_gram[word1][word2]:\n",
    "                      if word4 in self.quad_gram[word1][word2][word3]:\n",
    "                        acc+=math.log(self.quad_gram[word1][word2][word3][word4])\n",
    "                      elif \"__UNK\" in self.quad_gram[word1][word2][word3]:\n",
    "                        acc+=math.log(self.quad_gram[word1][word2][word3][\"__UNK\"])\n",
    "                    else:\n",
    "                      if word4 in self.quad_gram[word1][word2][\"__UNK\"]:\n",
    "                        acc+=math.log(self.quad_gram[word1][word2][\"__UNK\"][word4])\n",
    "                      elif \"__UNK\" in self.quad_gram[word1][word2][\"__UNK\"]:\n",
    "                        acc+=math.log(self.quad_gram[word1][word2][\"__UNK\"][\"__UNK\"])\n",
    "                  else:\n",
    "                    if \"__UNK\" in self.quad_gram[word1]:\n",
    "                      if word3 in self.quad_gram[word1][\"__UNK\"]:\n",
    "                        if word4 in self.quad_gram[word1][\"__UNK\"][word3]:\n",
    "                          acc+=math.log(self.quad_gram[word1][\"__UNK\"][word3][word4])\n",
    "                        elif \"__UNK\" in self.quad_gram[word1][\"__UNK\"][word3]:\n",
    "                          acc+=math.log(self.quad_gram[word1][\"__UNK\"][word3][\"__UNK\"])\n",
    "                      else:\n",
    "                        if \"__UNK\" in self.quad_gram[word1][\"__UNK\"]:\n",
    "                          if word4 in self.quad_gram[word1][\"__UNK\"][\"__UNK\"]:\n",
    "                            acc+=math.log(self.quad_gram[word1][\"__UNK\"][\"__UNK\"][word4])\n",
    "                          elif \"__UNK\" in self.quad_gram[word1][\"__UNK\"][\"__UNK\"]:\n",
    "                            acc+=math.log(self.quad_gram[word1][\"__UNK\"][\"__UNK\"][\"__UNK\"])\n",
    "                else:\n",
    "                  if \"__UNK\" in self.quad_gram:\n",
    "                    if word2 in self.quad_gram[\"__UNK\"]:\n",
    "                      if word3 in self.quad_gram[\"__UNK\"][word2]:\n",
    "                        if word4 in self.quad_gram[\"__UNK\"][word2][word3]:\n",
    "                          acc+=math.log(self.quad_gram[\"__UNK\"][word2][word3][word4])\n",
    "                        elif \"__UNK\" in self.quad_gram[\"__UNK\"][word2][word3]:\n",
    "                          acc+=math.log(self.quad_gram[\"__UNK\"][word2][word3][\"__UNK\"])\n",
    "                      else:\n",
    "                        if word4 in self.quad_gram[\"__UNK\"][word2][\"__UNK\"]:\n",
    "                          acc+=math.log(self.quad_gram[\"__UNK\"][word2][\"__UNK\"][word4])\n",
    "                        elif \"__UNK\" in self.quad_gram[\"__UNK\"][word2][\"__UNK\"]:\n",
    "                          acc+=math.log(self.quad_gram[\"__UNK\"][word2][\"__UNK\"][\"__UNK\"])\n",
    "                    else:\n",
    "                      if \"__UNK\" in self.quad_gram[\"__UNK\"]:\n",
    "                        if word3 in self.quad_gram[\"__UNK\"][\"__UNK\"]:\n",
    "                          if word4 in self.quad_gram[\"__UNK\"][\"__UNK\"][word3]:\n",
    "                            acc+=math.log(self.quad_gram[\"__UNK\"][\"__UNK\"][word3][word4])\n",
    "                          elif \"__UNK\" in self.quad_gram[\"__UNK\"][\"__UNK\"][word3]:\n",
    "                            acc+=math.log(self.quad_gram[\"__UNK\"][\"__UNK\"][word3][\"__UNK\"])\n",
    "                        else:\n",
    "                          if \"__UNK\" in self.quad_gram[\"__UNK\"][\"__UNK\"]:\n",
    "                            if word4 in self.quad_gram[\"__UNK\"][\"__UNK\"][\"__UNK\"]:\n",
    "                              acc+=math.log(self.quad_gram[\"__UNK\"][\"__UNK\"][\"__UNK\"][word4])\n",
    "                            elif \"__UNK\" in self.quad_gram[\"__UNK\"][\"__UNK\"][\"__UNK\"]:\n",
    "                              acc+=math.log(self.quad_gram[\"__UNK\"][\"__UNK\"][\"__UNK\"][\"__UNK\"])\n",
    "            return acc, len(tokens[1:])\n",
    "          except KeyError:\n",
    "            return acc, len(tokens[1:]) \n",
    "            \n",
    "    \n",
    "    def compute_probability(self,filenames=[],methodparams={}):\n",
    "        #computes the probability (and length) of a corpus contained in filenames\n",
    "        if filenames==[]:\n",
    "            filenames=self.files\n",
    "        total_p=0\n",
    "        total_N=0\n",
    "        for i,afile in enumerate(filenames):\n",
    "          if self.verbose:\n",
    "            print(\"Processing file {}:{}\".format(i,afile))\n",
    "          try:\n",
    "              with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                  for line in instream:\n",
    "                      line=line.rstrip()\n",
    "                      if len(line)>0:\n",
    "                          p,N=self.compute_prob_line(line,methodparams=methodparams)\n",
    "                          total_p+=p\n",
    "                          total_N+=N\n",
    "          except UnicodeDecodeError:\n",
    "            if self.verbose:\n",
    "              print(\"UnicodeDecodeError processing file {}: ignoring rest of file\".format(afile))\n",
    "            else:\n",
    "              pass\n",
    "        return total_p,total_N\n",
    "    \n",
    "    def compute_perplexity(self,filenames=[],methodparams={\"method\":\"bigram\",\"smoothing\":\"kneser-ney\"}):\n",
    "        \"\"\"\n",
    "        compute the probability and length of the corpus\n",
    "        calculate perplexity\n",
    "        lower perplexity means that the model better explains the data\n",
    "        \"\"\"\n",
    "        p,N=self.compute_probability(filenames=filenames,methodparams=methodparams)\n",
    "        # print(p,N)\n",
    "        if methodparams.get(\"method\") in [\"trigram\", \"quad_gram\"]:\n",
    "          rem = self.super_counter[methodparams.get(\"method\")] - self.magic_counter[methodparams.get(\"method\")]\n",
    "          pp=math.exp(-p/N) * (self.super_counter[methodparams.get(\"method\")]/rem)\n",
    "          return pp\n",
    "        pp=math.exp(-p/N)\n",
    "        return pp  \n",
    "    \n",
    "\n",
    "    def _make_unknowns(self,known=2):\n",
    "        \"\"\"\n",
    "        Method to distribute probability mass towards the unknown token.\n",
    "        param known (int): dictates cut off point where n-grams less frequent than known are pruned.\n",
    "        \"\"\"\n",
    "        # Unigram -----------------------------------\n",
    "        for (k,v) in list(self.unigram.items()):\n",
    "            if v<known:\n",
    "                del self.unigram[k]\n",
    "                self.unigram[\"__UNK\"]=self.unigram.get(\"__UNK\",0)+v\n",
    "        # Bigram -----------------------------------\n",
    "        for (k,adict) in list(self.bigram.items()):\n",
    "            for (kk,v) in list(adict.items()):\n",
    "                isknown=self.unigram.get(kk,0)\n",
    "                if isknown <= known:\n",
    "                    adict[\"__UNK\"]=adict.get(\"__UNK\",0)+v\n",
    "                    del adict[kk]\n",
    "            isknown=self.unigram.get(k,0)\n",
    "            if isknown <= known:\n",
    "                del self.bigram[k]\n",
    "                current=self.bigram.get(\"__UNK\",{})\n",
    "                current.update(adict)\n",
    "                self.bigram[\"__UNK\"]=current\n",
    "            else:\n",
    "                self.bigram[k]=adict\n",
    "        # Trigram -----------------------------------\n",
    "        for (k1, dict1) in list(self.trigram.items()):\n",
    "          for (k2, dict2) in list(dict1.items()):\n",
    "            for (k3, val) in list(dict2.items()):\n",
    "              isknown=self.unigram.get(k3,0)\n",
    "              if isknown == 0:\n",
    "                dict2[\"__UNK\"] = dict2.get(\"__UNK\",0) + val\n",
    "                del dict2[k3]\n",
    "            isknown=self.unigram.get(k2,0)\n",
    "            if isknown <= known:\n",
    "              del self.trigram[k1][k2]\n",
    "              current=self.trigram[k1].get(\"__UNK\",{})\n",
    "              current.update(dict2)\n",
    "              self.trigram[k1][\"__UNK\"] = current\n",
    "            else:\n",
    "              self.trigram[k1][k2] = dict2\n",
    "          # For first token:\n",
    "          isknown=self.unigram.get(k1,0)\n",
    "          if isknown <= known:\n",
    "            del self.trigram[k1]\n",
    "            current = self.trigram.get(\"__UNK\",{})\n",
    "            current.update(dict1)\n",
    "            self.trigram[\"__UNK\"] = current \n",
    "          else:\n",
    "            self.trigram[k1] = dict1\n",
    "        # Quad Gram -----------------------------------\n",
    "        for (k1, dict1) in list(self.quad_gram.items()):\n",
    "          for (k2, dict2) in list(dict1.items()):\n",
    "            for (k3, dict3) in list(dict2.items()):\n",
    "              for (k4, val) in list(dict3.items()):\n",
    "                # Next\n",
    "                isknown = self.unigram.get(k4,0)\n",
    "                if isknown <= known:\n",
    "                  dict3[\"__UNK\"] = dict3.get(\"__UNK\",0) + val\n",
    "                  del dict3[k4]\n",
    "              # Next\n",
    "              isknown=self.unigram.get(k3,0)\n",
    "              if isknown <= known:\n",
    "                del self.quad_gram[k1][k2][k3]\n",
    "                current = self.quad_gram[k1][k2].get(\"__UNK\", {})\n",
    "                current.update(dict3)\n",
    "                self.quad_gram[k1][k2][\"__UNK\"] = current\n",
    "              else:\n",
    "                self.quad_gram[k1][k2][k3] = dict3\n",
    "            # Next\n",
    "            isknown=self.unigram.get(k2,0)\n",
    "            if isknown <= known:\n",
    "              del self.quad_gram[k1][k2]\n",
    "              current = self.quad_gram[k1].get(\"__UNK\",{})\n",
    "              current.update(dict2)\n",
    "              self.quad_gram[k1][\"__UNK\"] = current\n",
    "            else:\n",
    "              self.quad_gram[k1][k2] = dict2\n",
    "          # Next\n",
    "          isknown=self.unigram.get(k1,0)\n",
    "          if isknown <= known:\n",
    "            del self.quad_gram[k1]\n",
    "            current = self.quad_gram.get(\"__UNK\", {})\n",
    "            current.update(dict1)\n",
    "            self.quad_gram[\"__UNK\"] = current\n",
    "          else:\n",
    "            self.quad_gram[k1] = dict1\n",
    "\n",
    "                \n",
    "    def _discount(self,discount=0.75):\n",
    "        #discount each bigram count by a small fixed amount\n",
    "        self.bigram={k:{kk:value-discount for (kk,value) in adict.items()}for (k,adict) in self.bigram.items()}\n",
    "        \n",
    "        #for each word, store the total amount of the discount so that the total is the same \n",
    "        #i.e., so we are reserving this as probability mass\n",
    "        for k in self.bigram.keys():\n",
    "            lamb=len(self.bigram[k])\n",
    "            self.bigram[k][\"__DISCOUNT\"]=lamb*discount\n",
    "            \n",
    "        #work out kneser-ney unigram probabilities\n",
    "        #count the number of contexts each word has been seen in\n",
    "        self.kn={}\n",
    "        for (k,adict) in self.bigram.items():\n",
    "            for kk in adict.keys():\n",
    "                self.kn[kk]=self.kn.get(kk,0)+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe7945cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uses gensim \n",
    "import gensim.downloader as api\n",
    "\n",
    "fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')\n",
    "word2vec_model300 = api.load('word2vec-google-news-300')\n",
    "glove_model300 = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f777e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processfiles(max_files=10):\n",
    "    \"\"\"\n",
    "    Code taken from n_gram_language_model class definition.\n",
    "    Returns lists of preprocessed and not tokenized sentences.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    sentences_preprocessed = []\n",
    "    for afile in tqdm.tqdm(training):\n",
    "        # print(\"Processing {}\".format(afile))\n",
    "        try:\n",
    "            with open(trainingdir,afile) as instream:\n",
    "                for line in instream:\n",
    "                    line=line.rstrip()\n",
    "                    if len(line)>0:\n",
    "                        tokens = [token for token in tokenize(line) if token not in stop_puncs]\n",
    "                        sentences_preprocessed.append(tokens)\n",
    "                        tokens = [token for token in tokenize(line)]\n",
    "                        sentences.append(tokens)\n",
    "        except UnicodeDecodeError:\n",
    "            print(\"\\nUnicodeDecodeError processing {}: ignoring rest of file\".format(afile))\n",
    "    return sentences, sentences_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c382c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261/261 [06:31<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "construct_params = {\n",
    "    \"known\" : 5,\n",
    "    \"verbose\" : False,\n",
    "    \"remove_stopwords\" : True\n",
    "}\n",
    "\n",
    "# Initialize n-gram language model.\n",
    "lm=n_gram_language_model(trainingdir=trainingdir,files=training, test_files=[], construct_params=construct_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1312d886",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_args = {\"n_gram_model\" : lm, \"pre_emb_model\" : fasttext_model300, \"ensemble\" : True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82789ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, csv\n",
    "questions=(\"testing_data.csv\")\n",
    "answers=(\"test_answer.csv\")\n",
    "\n",
    "with open(questions) as instream:\n",
    "    csvreader=csv.reader(instream)\n",
    "    lines=list(csvreader)\n",
    "qs_df=pd.DataFrame(lines[1:],columns=lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f860881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class question:\n",
    "\n",
    "    \"\"\"\n",
    "    Question class which stores information about a singular MSR SCC question.\n",
    "\n",
    "    Code taken from the original work of  Dr. J Weeds, University of Sussex.   \n",
    "   \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self,aline,stop=True):\n",
    "        self.fields=aline\n",
    "        self.num2letter = {\n",
    "            0:\"a\",\n",
    "            1:\"b\",\n",
    "            2:\"c\",\n",
    "            3:\"d\",\n",
    "            4:\"e\"\n",
    "            }\n",
    "        self.stop = stop\n",
    "        if self.stop:\n",
    "          self.tokenized = [token.lower() for token in tokenize(self.fields[1]) if token.lower() not in stop_puncs]\n",
    "          # self.tokenized = [wordnet_lemmatizer.lemmatize(token) for token in self.tokenized]\n",
    "        else:\n",
    "          self.tokenized = tokenize(self.fields[1])\n",
    "        self.options = self.fields[2:7]\n",
    "        self.backoff_factor = 0.4\n",
    "          \n",
    "\n",
    "    def get_field(self,field):\n",
    "        return self.fields[question.colnames[field]]\n",
    "    \n",
    "\n",
    "    def add_answer(self,fields):\n",
    "        self.answer=fields[1]\n",
    "   \n",
    "\n",
    "    def get_context(self,window,target=\"_____\",method=\"left\"):\n",
    "      \"\"\"\n",
    "      Method to return the context of a target word in question sentence. \n",
    "      If not sufficient context the method returns context with unknown token padding.\n",
    "      \"\"\"\n",
    "      for i, token in enumerate(self.tokenized):\n",
    "        if token == target:\n",
    "            if method==\"left\":\n",
    "              try:\n",
    "                return self.tokenized[i-window:i]\n",
    "              except:\n",
    "                return [\"__UNK\"] * window\n",
    "            elif method==\"right\": \n",
    "              return self.tokenized[i+1:i+1+window]\n",
    "\n",
    "\n",
    "    def chooseA(self):\n",
    "        return(\"a\")\n",
    "\n",
    "\n",
    "    def random(self):\n",
    "      \"\"\"\n",
    "      Retrun random choice of letter.\n",
    "      \"\"\"\n",
    "      return random.choice(self.num2letter)\n",
    "\n",
    "\n",
    "    def unigram(self):\n",
    "      \"\"\"\n",
    "      Return position of word with greatest unigram probability. 0 otherwise.\n",
    "      \"\"\"\n",
    "      option_probs = [lm.unigram[word] if word in lm.unigram else 0 for word in self.options]\n",
    "      index = option_probs.index(max(option_probs))\n",
    "      return self.num2letter[index]\n",
    "\n",
    "\n",
    "    def bigram(self, context_dir=\"left\"): # Backoff\n",
    "      \"\"\"\n",
    "      Return position of word-pair with greatest bigram probability. 0 otherwise. \n",
    "      \"\"\"\n",
    "      option_probs = []\n",
    "      context = self.get_context(1, method=context_dir) # [0] to delist context.\n",
    "      context = [\"__UNK\"] + context\n",
    "      if context_dir == \"left\":\n",
    "        for word in self.options:\n",
    "          # Bigram.\n",
    "          if context[-1] in lm.bigram and word in lm.bigram[context[-1]]:\n",
    "            option_probs.append(lm.bigram[context[-1]][word])\n",
    "          # Back off to unigram\n",
    "          elif word in lm.unigram:\n",
    "            option_probs.append(self.backoff_factor * lm.unigram[word])\n",
    "          else:\n",
    "            option_probs.append(0)\n",
    "      elif context_dir == \"right\":\n",
    "        option_probs = [lm.bigram[word][context] if word in lm.bigram and context in lm.bigram[word] else 0 for word in self.options]\n",
    "      index = option_probs.index(max(option_probs))\n",
    "      return self.num2letter[index]\n",
    "\n",
    "\n",
    "    def trigram(self, context_dir=\"left\"): # Backoff\n",
    "      \"\"\"\n",
    "      Return position of word-group with greatest trigram probability. 0 otherwise. \n",
    "      \"\"\"\n",
    "      option_probs = []\n",
    "      context = self.get_context(2, method=context_dir)\n",
    "      context = [\"__UNK\"] * 2 + context \n",
    "      if context_dir == \"left\":\n",
    "        for word in self.options:\n",
    "          if context[-2] in lm.trigram and context[-1] in lm.trigram[context[-2]] and word in lm.trigram[context[-2]][context[-1]]:\n",
    "            option_probs.append(lm.trigram[context[-2]][context[-1]][word])\n",
    "          # Back off to bigram.\n",
    "          elif context[-1] in lm.bigram and word in lm.bigram[context[-1]]:\n",
    "            option_probs.append(self.backoff_factor * lm.bigram[context[-1]][word])\n",
    "          # Back off to unigram.\n",
    "          elif word in lm.unigram:\n",
    "            option_probs.append(self.backoff_factor * self.backoff_factor * lm.unigram[word])\n",
    "          # Else 0.\n",
    "          else:\n",
    "            option_probs.append(0)\n",
    "      index = option_probs.index(max(option_probs))\n",
    "      return self.num2letter[index]\n",
    "\n",
    "\n",
    "    def quad_gram(self, context_dir=\"left\"): # Backoff\n",
    "      \"\"\"\n",
    "      Return position of word-group with greatest trigram probability. 0 otherwise. \n",
    "      \"\"\"\n",
    "      option_probs = []\n",
    "      context = self.get_context(3, method=context_dir)\n",
    "      context = [\"__UNK\"] * 3 + context\n",
    "      for word in self.options:\n",
    "        if context[-3] in lm.quad_gram and context[-2] in lm.quad_gram[context[-3]] and context[-1] in lm.quad_gram[context[-3]][context[-2]] and word in lm.quad_gram[context[-3]][context[-2]][context[-1]]:\n",
    "          option_probs.append(lm.quad_gram[context[-3]][context[-2]][context[-1]][word])\n",
    "        # Back off to trigram.\n",
    "        elif context[-2] in lm.trigram and context[-1] in lm.trigram[context[-2]] and word in lm.trigram[context[-2]][context[-1]]:\n",
    "          option_probs.append(self.backoff_factor * lm.trigram[context[-2]][context[-1]][word])\n",
    "        # Back off to bigram.\n",
    "        elif context[-1] in lm.bigram and word in lm.bigram[context[-1]]:\n",
    "          option_probs.append((self.backoff_factor**2) *lm.bigram[context[-1]][word])\n",
    "        # Back off to unigram.\n",
    "        elif word in lm.unigram:\n",
    "          option_probs.append((self.backoff_factor**3) * lm.unigram[word])\n",
    "        # Else 0.\n",
    "        else:\n",
    "          option_probs.append(0)\n",
    "      index = option_probs.index(max(option_probs))\n",
    "      return self.num2letter[index]\n",
    "   \n",
    "\n",
    "    def simple_4_gram(self, additional_args={}):\n",
    "      lm = additional_args.get(\"n_gram_model\")\n",
    "      option_probs = []\n",
    "      left_context = self.get_context(3, method=\"left\")\n",
    "      left_context = [\"__UNK\"] * 3 + left_context\n",
    "      con1 = left_context[-3]\n",
    "      con2 = left_context[-2]\n",
    "      con3 = left_context[-1]\n",
    "      right_context = self.get_context(3, method=\"right\")\n",
    "      right_context = right_context + [\"__UNK\"] * 3\n",
    "      r_con1 = right_context[0]\n",
    "      r_con2 = right_context[1]\n",
    "      r_con3 = right_context[2]\n",
    "      for word in self.options:\n",
    "        try:\n",
    "          score = 0\n",
    "          # Bigram\n",
    "          if word in lm.bigram.get(con3,{}):\n",
    "            score += 1\n",
    "          if r_con1 in lm.bigram.get(word,{}):\n",
    "            score += 1\n",
    "          # Trigram\n",
    "          if word in lm.trigram.get(con3,{}).get(con2,{}):\n",
    "            score += 2\n",
    "          if r_con1 in lm.trigram.get(con3,{}).get(word,{}):\n",
    "            score += 2\n",
    "          if r_con2 in lm.trigram.get(word,{}).get(r_con1,{}):\n",
    "            score += 2\n",
    "          # Quad_gram\n",
    "          if word in lm.quad_gram.get(con1,{}).get(con2,{}).get(con3,{}):\n",
    "            score += 3\n",
    "          if r_con1 in lm.quad_gram.get(con2,{}).get(con3,{}).get(word,{}):\n",
    "            score += 3\n",
    "          if r_con2 in lm.quad_gram.get(con3,{}).get(word,{}).get(r_con1,{}):\n",
    "            score += 3\n",
    "          if r_con3 in lm.quad_gram.get(word,{}).get(r_con1,{}).get(r_con2,{}):\n",
    "            score += 3\n",
    "          option_probs.append(score)\n",
    "        except TypeError:\n",
    "          print([con1,con2,con3,word,r_con1,r_con2,r_con3])\n",
    "          option_probs.append(0)\n",
    "      # -------------\n",
    "      # Ensemble\n",
    "      if additional_args.get(\"ensemble\", False):\n",
    "        return option_probs\n",
    "      # -------------\n",
    "      index = option_probs.index(max(option_probs))\n",
    "      return self.num2letter[index]\n",
    "\n",
    "\n",
    "    def embedding_similarity(self, method=\"cos\", additional_args={}):\n",
    "      \"\"\"\n",
    "      For use with pretrained or even custom embeddings.\n",
    "      \"\"\"\n",
    "      model = additional_args.get(\"pre_emb_model\")\n",
    "      option_probs = []\n",
    "      # Remove target string.\n",
    "      sentence = self.tokenized #.remove(\"_____\")\n",
    "      # Iterate through candidate choices.\n",
    "      for word in self.options:\n",
    "        try:\n",
    "          # If no embedding for that word exists.\n",
    "          if word not in model:\n",
    "            option_probs.append(0)\n",
    "            # Continue to next candidate word.\n",
    "            continue\n",
    "          # Get vectorized form of word.\n",
    "          word_vector = model.get_vector(word)\n",
    "          # Get vectorized form of sentence tokens.\n",
    "          sentence_vectors = [model.get_vector(sent_token) for sent_token in sentence if sent_token in model and sent_token != \"_____\"]\n",
    "          # For euclidean distances.\n",
    "          if method == \"euc\":\n",
    "            sim_score = [np.linalg.norm(model.get_vector(word) - model.get_vector(sent_token)) for sent_token in sentence if sent_token in model and sent_token != \"_____\"]\n",
    "          # For cosine distances.\n",
    "          else:\n",
    "            sim_score = model.cosine_similarities(word_vector, sentence_vectors)\n",
    "          # Append average \"method\" similarity.\n",
    "          option_probs.append(sum(sim_score)/len(sim_score))\n",
    "        except (TypeError, np.AxisError, ZeroDivisionError) as e:\n",
    "          print(sentence)\n",
    "          option_probs.append(0)\n",
    "      # ----------------\n",
    "      # Ensemble - cosine:\n",
    "      if additional_args.get(\"ensemble\", False):\n",
    "        return option_probs\n",
    "      # ----------------\n",
    "      if method == \"cos\":\n",
    "        index = option_probs.index(max(option_probs))\n",
    "      else:\n",
    "        index = option_probs.index(min(option_probs))\n",
    "      return self.num2letter[index]\n",
    "\n",
    "\n",
    "    def ensemble(self, additional_args={}):\n",
    "      \"\"\"\n",
    "      Ensemble method which aggregates scores of both tested models and normalizes + sums them.\n",
    "      \"\"\"\n",
    "      n_gram_model = additional_args.get(\"n_gram_model\")\n",
    "      n_gram = self.simple_4_gram(additional_args=additional_args)\n",
    "      norm_n_gram = [float(i)/sum(n_gram) if sum(n_gram) !=0 else 0 for i in n_gram]\n",
    "\n",
    "      pre_emb_model = additional_args.get(\"pre_emb_model\")\n",
    "      pre_emb = self.embedding_similarity(additional_args=additional_args)\n",
    "      norm_pre_emb = [float(i)/sum(pre_emb) if sum(pre_emb) !=0 else 0 for i in pre_emb]\n",
    "\n",
    "      option_probs = [sum(val) for val in zip(norm_n_gram, norm_pre_emb)]\n",
    "      index = option_probs.index(max(option_probs))\n",
    "      return self.num2letter[index]\n",
    "\n",
    "\n",
    "  # ################################################### comment below to use stupid backoff.\n",
    "    def bigram(self, context_dir=\"left\"):\n",
    "      \"\"\"\n",
    "      Return position of word-pair with greatest bigram probability. 0 otherwise. \n",
    "      \"\"\"\n",
    "      try:\n",
    "        context = self.get_context(1, method=context_dir)[0] # [0] to delist context.\n",
    "      except:\n",
    "        context = [\"__START\"][0] \n",
    "      if context_dir == \"left\":\n",
    "        option_probs = [lm.bigram[context][word] if context in lm.bigram and word in lm.bigram[context] else 0 for word in self.options]\n",
    "      elif context_dir == \"right\":\n",
    "        option_probs = [lm.bigram[word][context] if word in lm.bigram and context in lm.bigram[word] else 0 for word in self.options]\n",
    "      index = option_probs.index(max(option_probs))\n",
    "      return self.num2letter[index]\n",
    "\n",
    "\n",
    "    def trigram(self, context_dir=\"left\"): \n",
    "      \"\"\"\n",
    "      Return position of word-group with greatest trigram probability. 0 otherwise. \n",
    "      \"\"\"\n",
    "      option_probs = []\n",
    "      try:\n",
    "        context = [\"__UNK\"] * 2 + self.get_context(2, method=context_dir)\n",
    "      except:\n",
    "        context = [\"__UNK\"] * 2 + context\n",
    "      if context_dir == \"left\":\n",
    "        for word in self.options:\n",
    "          if context[-2] in lm.trigram and context[-1] in lm.trigram[context[-2]] and word in lm.trigram[context[-2]][context[-1]]:\n",
    "            option_probs.append(lm.trigram[context[-2]][context[-1]][word])\n",
    "          elif context[-2] in lm.trigram and context[-1] in lm.trigram[context[-2]] and \"__UNK\" in lm.trigram[context[-2]][context[-1]]:\n",
    "            option_probs.append(lm.trigram[context[-2]][context[-1]][\"__UNK\"])\n",
    "          # Else 0.\n",
    "          else:\n",
    "            option_probs.append(0)\n",
    "      index = option_probs.index(max(option_probs))\n",
    "      return self.num2letter[index]\n",
    "\n",
    "\n",
    "    def quad_gram(self, context_dir=\"left\"):\n",
    "      \"\"\"\n",
    "      Return position of word-group with greatest trigram probability. 0 otherwise. \n",
    "      \"\"\"\n",
    "      option_probs = []\n",
    "      context = [\"__UNK\"] * 3 + self.get_context(3, method=context_dir)\n",
    "      con_len = len(context)\n",
    "      for word in self.options:\n",
    "        if context[-3] in lm.quad_gram and context[-2] in lm.quad_gram[context[-3]] and context[-1] in lm.quad_gram[context[-3]][context[-2]] and word in lm.quad_gram[context[-3]][context[-2]][context[-1]]:\n",
    "          option_probs.append(lm.quad_gram[context[-3]][context[-2]][context[-1]][word])\n",
    "        elif context[-3] in lm.quad_gram and context[-2] in lm.quad_gram[context[-3]] and context[-1] in lm.quad_gram[context[-3]][context[-2]] and \"__UNK\" in lm.quad_gram[context[-3]][context[-2]][context[-1]]:\n",
    "          option_probs.append(lm.quad_gram[context[-3]][context[-2]][context[-1]][\"__UNK\"])\n",
    "        else:\n",
    "          option_probs.append(0)\n",
    "      index = option_probs.index(max(option_probs))\n",
    "      return self.num2letter[index]\n",
    "    \n",
    "    def quad_gram(self, context_dir=\"left\"):\n",
    "      \"\"\"\n",
    "      Return position of word-group with greatest trigram probability. 0 otherwise. \n",
    "      \"\"\"\n",
    "      option_probs = []\n",
    "      context = [\"__UNK\"] * 3 + self.get_context(3, method=context_dir)\n",
    "      con1, con2, con3 = context[-3], context[-2], context[-1]\n",
    "      try:\n",
    "        if con1 not in lm.quad_gram:\n",
    "          con1 = \"__UNK\"\n",
    "        if con2 not in lm.quad_gram[con1]:\n",
    "          con2 = \"__UNK\"\n",
    "        if con3 not in lm.quad_gram[con1][con2]:\n",
    "          con3 = \"__UNK\"\n",
    "        for word in self.options:\n",
    "          if con1 in lm.quad_gram and con2 in lm.quad_gram[con1] and con3 in lm.quad_gram[con1][con2] and word in lm.quad_gram[con1][con2][con3]:\n",
    "            option_probs.append(lm.quad_gram[con1][con2][con3][word])\n",
    "          elif con1 in lm.quad_gram and con2 in lm.quad_gram[con1] and con3 in lm.quad_gram[con1][con2] and \"__UNK\" in lm.quad_gram[con1][con2][con3]:\n",
    "            option_probs.append(lm.quad_gram[con1][con2][con3][\"__UNK\"])\n",
    "          else:\n",
    "            option_probs.append(0)\n",
    "      except KeyError:\n",
    "        option_probs.append(0)\n",
    "      index = option_probs.index(max(option_probs))\n",
    "      return self.num2letter[index]\n",
    "  # ################################################### \n",
    "    \n",
    "    \n",
    "    def predict(self, method=\"chooseA\", additional_args=None):\n",
    "        if method==\"chooseA\":\n",
    "          return self.chooseA()\n",
    "        elif method==\"random\":\n",
    "          return self.random()\n",
    "        elif method==\"unigram\":\n",
    "          return self.unigram()\n",
    "        elif method==\"bigram\":\n",
    "          return self.bigram()\n",
    "        elif method==\"trigram\":\n",
    "          return self.trigram()\n",
    "        elif method==\"quad_gram\":\n",
    "          return self.quad_gram()\n",
    "        elif method==\"simple_4_gram\":\n",
    "          return self.simple_4_gram(additional_args=additional_args)\n",
    "        elif method==\"embedding_similarity\":\n",
    "          return self.embedding_similarity(additional_args=additional_args)\n",
    "        elif method==\"ensemble\":\n",
    "          return self.ensemble(additional_args=additional_args)\n",
    "        elif method == \"cos\":\n",
    "          return self.embedding_similarity(additional_args=additional_args)\n",
    "        elif method == \"euc\":\n",
    "          return self.embedding_similarity(additional_args=args,method=\"euc\")\n",
    "\n",
    "\n",
    "    def predict_and_score(self, method=\"chooseA\", additional_args=None):\n",
    "        #compare prediction according to method with the correct answer\n",
    "        #return 1 or 0 accordingly\n",
    "        # Method also records which questions were answered correctly by index.\n",
    "        prediction=self.predict(method=method, additional_args=additional_args)\n",
    "        if prediction == self.answer:\n",
    "            correct_answers.get(method).append(1)\n",
    "            return 1\n",
    "        else:\n",
    "            correct_answers.get(method).append(0)\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d072beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class scc_reader:\n",
    "    \n",
    "\n",
    "    def __init__(self,qs=questions,ans=answers,stop=False):\n",
    "        self.qs=qs\n",
    "        self.ans=ans\n",
    "        self.stop = stop\n",
    "        self.read_files()\n",
    "\n",
    "\n",
    "    def read_files(self):\n",
    "        \n",
    "        #read in the question file\n",
    "        with open(self.qs) as instream:\n",
    "            csvreader=csv.reader(instream)\n",
    "            qlines=list(csvreader)\n",
    "        \n",
    "        #store the column names as a reverse index so they can be used to reference parts of the question\n",
    "        question.colnames={item:i for i,item in enumerate(qlines[0])}\n",
    "        \n",
    "        #create a question instance for each line of the file (other than heading line)\n",
    "        self.questions=[question(qline, self.stop) for qline in qlines[1:]]\n",
    "        \n",
    "        #read in the answer file\n",
    "        with open(self.ans) as instream:\n",
    "            csvreader=csv.reader(instream)\n",
    "            alines=list(csvreader)\n",
    "            \n",
    "        #add answers to questions so predictions can be checked    \n",
    "        for q,aline in zip(self.questions,alines[1:]):\n",
    "            q.add_answer(aline)\n",
    "\n",
    "\n",
    "    def get_field(self,field):\n",
    "        return [q.get_field(field) for q in self.questions] \n",
    "    \n",
    "\n",
    "    def predict(self,method=\"chooseA\"):\n",
    "        return [q.predict(method=method) for q in self.questions]\n",
    "    \n",
    "    def predict_and_score(self,method=\"chooseA\", additional_args=None):\n",
    "        scores=[q.predict_and_score(method=method, additional_args=additional_args) for q in self.questions]\n",
    "        return sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be2dd182",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_answers = {\"simple_4_gram\" : [], \"embedding_similarity\" : [], \"ensemble\" : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8501957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35096153846153844"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple-4-gram score\n",
    "additional_args = {\"n_gram_model\" : lm, \"pre_emb_model\" : fasttext_model300, \"ensemble\" : False}\n",
    "SCC = scc_reader(questions, answers, stop=True)\n",
    "SCC.predict_and_score(method=\"simple_4_gram\", additional_args=additional_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bd27cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_____']\n",
      "['_____']\n",
      "['_____']\n",
      "['_____']\n",
      "['_____']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.43173076923076925"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding similarity score\n",
    "SCC = scc_reader(questions, answers, stop=True)\n",
    "SCC.predict_and_score(method=\"embedding_similarity\", additional_args=additional_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f98b7fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_____']\n",
      "['_____']\n",
      "['_____']\n",
      "['_____']\n",
      "['_____']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.43846153846153846"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensemble score\n",
    "additional_args[\"ensemble\"] = True\n",
    "SCC = scc_reader(questions, answers, stop=True)\n",
    "SCC.predict_and_score(method=\"ensemble\", additional_args=additional_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bbb15b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the answer file\n",
    "holding_list = []\n",
    "with open(\"test_answer.csv\") as instream:\n",
    "    csvreader=csv.reader(instream)\n",
    "    alines=list(csvreader)\n",
    "    holding_list.append(alines)\n",
    "answers = [holding_list[0][i][1] for i, _ in enumerate(holding_list[0]) if i != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abac38ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All correct.\n",
    "correct = []\n",
    "# All incorrect.\n",
    "incorrect = []\n",
    "# N-gram incorrect\n",
    "n_g_incorrect = []\n",
    "# n-g over emb.\n",
    "n_g_vs_emb = []\n",
    "# emb over n-g.\n",
    "emb_vs_n_g = []\n",
    "# ensemble got right when other two didnt.\n",
    "ensemble_solved = []\n",
    "# Ensemble favouring n-gram.\n",
    "n_g_favour = []\n",
    "# Ensemble favouring word embeddings.\n",
    "emb_favour = []\n",
    "# Ensemble only.\n",
    "ensemble_only = []\n",
    "\n",
    "for i, (n_g, emb, ensemble) in enumerate(zip(correct_answers[\"simple_4_gram\"], correct_answers[\"embedding_similarity\"], correct_answers[\"ensemble\"])):\n",
    "  if n_g == emb == ensemble == 1:\n",
    "    correct.append(i)\n",
    "  if n_g == emb == ensemble == 0:\n",
    "    incorrect.append(i)\n",
    "  if n_g == 1 and emb == 0:\n",
    "    n_g_vs_emb.append(i)\n",
    "  if n_g == 0 and emb == 1:\n",
    "    emb_vs_n_g.append(i)\n",
    "  if n_g == emb == 0 and ensemble == 1:\n",
    "    ensemble_solved.append(i)\n",
    "  if n_g == 0:\n",
    "    n_g_incorrect.append(i)\n",
    "\n",
    "  if n_g == 0 and ensemble == 1 == emb:\n",
    "    emb_favour.append(i)\n",
    "  if emb == 0 and ensemble == 1 == n_g:\n",
    "    n_g_favour.append(i)\n",
    "\n",
    "  if ensemble == 1:\n",
    "    ensemble_only.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "879e0a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(index):\n",
    "  \n",
    "  q = question([SCC.get_field(\"id\")[index], SCC.get_field(\"question\")[index], SCC.get_field(\"a)\")[index], SCC.get_field(\"b)\")[index],SCC.get_field(\"c)\")[index], SCC.get_field(\"d)\")[index], SCC.get_field(\"e)\")[index]])\n",
    "  answer = answers[index]\n",
    "  \n",
    "  additional_args = {\"n_gram_model\" : lm, \"pre_emb_model\" : fasttext_model300, \"ensemble\" : False}\n",
    "  pred_ngram = q.predict(\"simple_4_gram\", additional_args=additional_args)\n",
    "  answer_ngram = SCC.get_field(\"{})\".format(pred_ngram))[index]\n",
    "  \n",
    "  pred_emb = q.predict(\"embedding_similarity\", additional_args=additional_args)\n",
    "  answer_emb = SCC.get_field(\"{})\".format(pred_emb))[index]\n",
    "  \n",
    "  additional_args[\"ensemble\"] = True\n",
    "  pred_ensemble = q.predict(\"ensemble\", additional_args=additional_args)\n",
    "  answer_ensemble = SCC.get_field(\"{})\".format(pred_ensemble))[index]\n",
    "\n",
    "  answer_correct = SCC.get_field(\"{})\".format(answers[index]))[index]\n",
    "\n",
    "  print(\"------------------------------\")\n",
    "  print(SCC.get_field(\"question\")[index])\n",
    "  print(q.tokenized)\n",
    "  print()\n",
    "  print(answer_correct)\n",
    "  print()\n",
    "  print(\"N-gram: {}\".format(answer_ngram))\n",
    "  print(\"Embedd: {}\".format(answer_emb))\n",
    "  print(\"Ensemb: {}\".format(answer_ensemble))\n",
    "  print(\"------------------------------\")\n",
    "  print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3e138bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "My mistress told me that you were _____ to call.\n",
      "['mistress', 'told', '_____', 'call']\n",
      "\n",
      "likely\n",
      "\n",
      "N-gram: running\n",
      "Embedd: written\n",
      "Ensemb: likely\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "He may have been _____ , or he may have been so paralyzed with terror as to have been unable to cry out.\n",
      "['may', '_____', 'may', 'paralyzed', 'terror', 'unable', 'cry']\n",
      "\n",
      "asleep\n",
      "\n",
      "N-gram: abroad\n",
      "Embedd: misinformed\n",
      "Ensemb: asleep\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "His slippers , too , were gone , but his _____ were left behind.\n",
      "['slippers', 'gone', '_____', 'left', 'behind']\n",
      "\n",
      "boots\n",
      "\n",
      "N-gram: feelings\n",
      "Embedd: looks\n",
      "Ensemb: boots\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "He was tractable enough , though his son was a perfect _____ , ready to blow out his own or anybody else's brains if he could have got to his revolver.\n",
      "['tractable', 'enough', 'though', 'son', 'perfect', '_____', 'ready', 'blow', 'anybody', 'else', \"'s\", 'brains', 'could', 'got', 'revolver']\n",
      "\n",
      "demon\n",
      "\n",
      "N-gram: knight\n",
      "Embedd: lad\n",
      "Ensemb: demon\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "It was _____ to me that he could have gone away leaving all his property behind him , and yet where could he be.\n",
      "['_____', 'could', 'gone', 'away', 'leaving', 'property', 'behind', 'yet', 'could']\n",
      "\n",
      "incredible\n",
      "\n",
      "N-gram: agreeable\n",
      "Embedd: essential\n",
      "Ensemb: incredible\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "That is my room at the end of the _____ , and my son's is the one beyond it.\n",
      "['room', 'end', '_____', 'son', \"'s\", 'one', 'beyond']\n",
      "\n",
      "stairs\n",
      "\n",
      "N-gram: cliff\n",
      "Embedd: water-side\n",
      "Ensemb: stairs\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "An inspection of his chair showed me that he had been in the _____ of standing on it , which of course would be necessary in order that he should reach the ventilator.\n",
      "['inspection', 'chair', 'showed', '_____', 'standing', 'course', 'would', 'necessary', 'order', 'reach', 'ventilator']\n",
      "\n",
      "habit\n",
      "\n",
      "N-gram: depth\n",
      "Embedd: style\n",
      "Ensemb: habit\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "And what deep and earnest purpose can he have which _____ for such a trial.\n",
      "['deep', 'earnest', 'purpose', '_____', 'trial']\n",
      "\n",
      "calls\n",
      "\n",
      "N-gram: played\n",
      "Embedd: ceased\n",
      "Ensemb: calls\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "I had no _____ to tell the baronet what I had learned about Mrs. Lyons upon the evening before , for Dr. Mortimer remained with him at cards until it was very late.\n",
      "['_____', 'tell', 'baronet', 'learned', 'mrs.', 'lyons', 'upon', 'evening', 'dr.', 'mortimer', 'remained', 'cards', 'late']\n",
      "\n",
      "opportunity\n",
      "\n",
      "N-gram: impulse\n",
      "Embedd: pretensions\n",
      "Ensemb: opportunity\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "Before entering , Holmes made an _____ of the door which had been forced.\n",
      "['entering', 'holmes', 'made', '_____', 'door', 'forced']\n",
      "\n",
      "examination\n",
      "\n",
      "N-gram: estimate\n",
      "Embedd: access\n",
      "Ensemb: examination\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "I am sure that if you _____ my name they would be happy to show it to you.\n",
      "['sure', '_____', 'name', 'would', 'happy', 'show']\n",
      "\n",
      "mentioned\n",
      "\n",
      "N-gram: killed\n",
      "Embedd: preferred\n",
      "Ensemb: mentioned\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "He unwound the _____ and held out his hand.\n",
      "['unwound', '_____', 'held', 'hand']\n",
      "\n",
      "handkerchief\n",
      "\n",
      "N-gram: gun\n",
      "Embedd: saddle\n",
      "Ensemb: handkerchief\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "His eyes rolled upwards , his features writhed in agony , and with a suppressed groan he _____ on his face upon the ground.\n",
      "['eyes', 'rolled', 'upwards', 'features', 'writhed', 'agony', 'suppressed', 'groan', '_____', 'face', 'upon', 'ground']\n",
      "\n",
      "dropped\n",
      "\n",
      "N-gram: leaned\n",
      "Embedd: staggered\n",
      "Ensemb: dropped\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "His whole face sharpened away into nose and chin , and the skin of his cheeks was _____ quite tense over his outstanding bones.\n",
      "['whole', 'face', 'sharpened', 'away', 'nose', 'chin', 'skin', 'cheeks', '_____', 'quite', 'tense', 'outstanding', 'bones']\n",
      "\n",
      "drawn\n",
      "\n",
      "N-gram: falling\n",
      "Embedd: drooped\n",
      "Ensemb: drawn\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "You were dwelling upon the sadness and horror and _____ waste of life.\n",
      "['dwelling', 'upon', 'sadness', 'horror', '_____', 'waste', 'life']\n",
      "\n",
      "useless\n",
      "\n",
      "N-gram: humble\n",
      "Embedd: peaceful\n",
      "Ensemb: useless\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "Holmes shrugged his shoulders with a glance of comic resignation towards the Colonel , and the talk _____ away into less dangerous channels.\n",
      "['holmes', 'shrugged', 'shoulders', 'glance', 'comic', 'resignation', 'towards', 'colonel', 'talk', '_____', 'away', 'less', 'dangerous', 'channels']\n",
      "\n",
      "drifted\n",
      "\n",
      "N-gram: rode\n",
      "Embedd: rumbled\n",
      "Ensemb: drifted\n",
      "------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[error_analysis(index) for index in ensemble_solved]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fffc6f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [02:55<00:00,  2.19s/it] \n",
      "100%|██████████| 80/80 [02:27<00:00,  1.85s/it]\n",
      "100%|██████████| 80/80 [4:08:45<00:00, 186.57s/it]    \n",
      "100%|██████████| 80/80 [10:24<00:00,  7.80s/it] \n"
     ]
    }
   ],
   "source": [
    "# Example development code for experimenting with the known parameter.\n",
    "lm_known = {}\n",
    "MAX_FILES=100\n",
    "\n",
    "training_shuffled = shuffle(training)\n",
    "training_shuffled = training_shuffled[:MAX_FILES]\n",
    "\n",
    "num = 0.2\n",
    "train, test = train_test_split(training_shuffled,test_size=num)\n",
    "\n",
    "for known in [5, 10, 25, 50]:\n",
    "\n",
    "\n",
    "  construct_params = {\n",
    "      \"known\" : known,\n",
    "      \"verbose\" : False,\n",
    "      \"remove_stopwords\" : True\n",
    "  }\n",
    "\n",
    "  # Initialize n-gram language model.\n",
    "  lm=n_gram_language_model(trainingdir=trainingdir,files=train, test_files=test, construct_params=construct_params)\n",
    "\n",
    "  lm_known[known] = lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1845b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowns_test = [lm.compute_perplexity(filenames=lm.test_files,methodparams={\"method\":method}) for lm in lm_stop.values() for method in [\"unigram\", \"bigram\", \"trigram\", \"quad_gram\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91efa20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(lst, n):\n",
    "    \"\"\"\n",
    "    Chunnk lst (list) into n chunks.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112f15c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = [len(lm.unigram) for lm in lm_stop.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff32ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = [len(lm.unigram) for lm in lm_stop.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5832dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preproc = pd.DataFrame({\n",
    "    'Training Doc Size': [key for key, lm in lm_stop.items()], \n",
    "    'unigram': one,\n",
    "    'bigram': two,\n",
    "    \"trigram\":three,\n",
    "    '4-gram': four,\n",
    "    'vocab': vocab_size,\n",
    "    })\n",
    "\n",
    "data_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e427cf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph Reprasantation.\n",
    "ax = sns.lineplot(x='knowns', y='value', hue='variable', \n",
    "             data=pd.melt(data_preproc, ['knowns']))\n",
    "\n",
    "ax.set(xlabel=\"Known Parameter Value\", ylabel=\"Perplexity\")\n",
    "# ax._legend.set_title(\"N-Gram\")\n",
    "leg = ax.legend()\n",
    "leg.set_title(\"N-Gram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906c0a6c",
   "metadata": {},
   "source": [
    "Refrences:-\n",
    "1.LAB resources of Dr julie weeds from University of sussex.\n",
    "\n",
    "2.https://github.com/LordLean/Microsoft-Research-Sentence-Completion-Challenge/blob/main/MSRSCC.ipynb From github the code imported.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
